{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import src.util as util\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from src import _PreprocessingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = util.load_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config_data: dict, config: str) -> pd.DataFrame:\n",
    "    # Load set of data\n",
    "    X_train = util.pickle_load(config_data[\"train_set_eda\"][0])\n",
    "    y_train = util.pickle_load(config_data[\"train_set_eda\"][1])\n",
    "    \n",
    "    X_valid = util.pickle_load(config_data[\"valid_set_eda\"][0])\n",
    "    y_valid = util.pickle_load(config_data[\"valid_set_eda\"][1])\n",
    "    \n",
    "    X_test = util.pickle_load(config_data[\"test_set_eda\"][0])\n",
    "    y_test = util.pickle_load(config_data[\"test_set_eda\"][1])\n",
    "    \n",
    "    dataset_train = pd.concat([X_train[config], y_train[config]], axis=1)\n",
    "    dataset_valid = pd.concat([X_valid[config], y_valid[config]], axis=1)\n",
    "    dataset_test = pd.concat([X_test[config], y_test[config]], axis=1)\n",
    "    \n",
    "    \n",
    "    return dataset_train, dataset_valid, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set, valid_set, test_set = load_dataset(config_data, config='rf')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Preprocessing data\n",
    "\n",
    "- Split dataset into numerical and categorical data, <br>\n",
    "- Performed **SimpleImputer** in order of missing data <br>\n",
    "- Performed **Encoding** for categorical data such as **LabelEncoder** for ordinal data and **OHE** for non-ordinal data <br>\n",
    "- Suggested to use **OrdinalEncoder** and defined the ranked value of each features <br>\n",
    "- Last, performed **Standardization** using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _handling_data:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def _split_xy(self, data:dict) -> pd.DataFrame:\n",
    "        X_data = data.drop(columns = config_data['label'], axis=1)\n",
    "        y_data = data[config_data['label']]\n",
    "        \n",
    "        self.X = X_data\n",
    "        self.y = y_data\n",
    "        \n",
    "        numerical_col = X_data.select_dtypes('float64').columns.to_list()\n",
    "        categorical_col = X_data.select_dtypes('object').columns.to_list()\n",
    "\n",
    "        X_num = X_data[numerical_col]\n",
    "        X_cat = X_data[categorical_col]\n",
    "\n",
    "        return  X_num, X_cat\n",
    "    \n",
    "    # Perform sanity check\n",
    "    def _imputer_Num(self, data, imputer=None):\n",
    "        if imputer == None:\n",
    "            imputer = SimpleImputer(missing_values=np.nan,\n",
    "                                    strategy='median')\n",
    "            imputer.fit(data)\n",
    "\n",
    "        data_imputed_num = pd.DataFrame(imputer.transform(data),\n",
    "                                    index = data.index,\n",
    "                                    columns = data.columns)\n",
    "        \n",
    "        data_imputed_num = data_imputed_num.astype('int64')\n",
    "        \n",
    "        self.data_imputed_num = data_imputed_num\n",
    "        \n",
    "        return data_imputed_num\n",
    "\n",
    "    def _imputer_Cat(self, data, imputer = None) -> pd.DataFrame:\n",
    "        if imputer == None:\n",
    "            imputer = SimpleImputer(missing_values=np.nan,\n",
    "                                    strategy='most_frequent')\n",
    "            imputer.fit(data)\n",
    "\n",
    "        data_imputed_cat = pd.DataFrame(imputer.transform(data),\n",
    "                                    index=data.index,\n",
    "                                    columns=data.columns\n",
    "                                    )\n",
    "        \n",
    "        self.data_imputed_cat = data_imputed_cat\n",
    "        \n",
    "        return data_imputed_cat\n",
    "    \n",
    "    def _OHE_Cat(self, data, encoder_col = None, encoder = None) -> pd.DataFrame:\n",
    "\n",
    "        if encoder == None:\n",
    "            encoder = OneHotEncoder(handle_unknown= 'ignore',\n",
    "                                    drop = 'if_binary')\n",
    "            encoder.fit(data)\n",
    "            encoder_col = encoder.get_feature_names_out(data.columns)\n",
    "\n",
    "        data_encoded = encoder.transform(data).toarray()\n",
    "        data_encoded = pd.DataFrame(data_encoded,\n",
    "                                    index=data.index,\n",
    "                                    columns=encoder_col)\n",
    "        \n",
    "        # util.pickle_dump(encoder, config_data[\"ohe_path\"])\n",
    "        \n",
    "        return data_encoded, encoder_col, encoder\n",
    "    \n",
    "    def _LE_cat(self, data, encoder = None) -> pd.DataFrame:\n",
    "        \n",
    "        if encoder == None:\n",
    "            le_encoder = LabelEncoder()\n",
    "            for col in data.columns.to_list():\n",
    "                data[col] = le_encoder.fit_transform(data[col])\n",
    "        \n",
    "        return data, le_encoder\n",
    "    \n",
    "    def _concat_data(self, nominal, ordinal_ohe=None, ordinal_le=None):\n",
    "        \n",
    "        if ordinal_ohe is not None and not ordinal_ohe.empty:\n",
    "            X_train_ohe, encoder_ohe_col, encoder_ohe = self._OHE_Cat(ordinal_ohe)\n",
    "            X_train_ = pd.concat([nominal, X_train_ohe], axis=1)\n",
    "            \n",
    "        if ordinal_le is not None and not ordinal_le.empty:\n",
    "            X_train_le, encoder_le = self._LE_cat(data=ordinal_le)\n",
    "            X_train_ = pd.concat([nominal, X_train_le], axis=1)\n",
    "        \n",
    "        if ordinal_ohe is not None and not ordinal_ohe.empty and ordinal_le is not None and not ordinal_le.empty:\n",
    "            X_train_ohe, encoder_ohe_col, encoder_ohe = self._OHE_Cat(ordinal_ohe)\n",
    "            X_train_le, encoder_le = self._LE_cat(data=ordinal_le)\n",
    "            \n",
    "            X_train_cat_concat = pd.concat([X_train_ohe, X_train_le], axis=1)\n",
    "            X_train_ = pd.concat([nominal, X_train_cat_concat], axis=1)\n",
    "            \n",
    "        return X_train_\n",
    "    \n",
    "    def _standardize_Data(self, data, scaler=None) -> pd.DataFrame:\n",
    "        if scaler == None:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(data)\n",
    "\n",
    "        data_scaled = pd.DataFrame(scaler.transform(data),\n",
    "                                index=data.index,\n",
    "                                columns=data.columns)\n",
    "        \n",
    "        return data_scaled, scaler\n",
    "    \n",
    "    def _handling_data(self, data, encoding='le'):\n",
    "        \n",
    "        X_num, X_cat = self._split_xy(data)\n",
    "        \n",
    "        X_num = self._imputer_Num(data=X_num)\n",
    "        X_cat = self._imputer_Cat(data=X_cat)\n",
    "        \n",
    "        if encoding == 'le':\n",
    "            X_ = self._concat_data(nominal=X_num,\n",
    "                                   ordinal_le=X_cat)\n",
    "            \n",
    "        elif encoding == 'ohe':\n",
    "            X_ = self._concat_data(nominal=X_num,\n",
    "                                   ordinal_ohe=X_cat)\n",
    "        \n",
    "        else:\n",
    "            X_ = self._concat_data(nominal=X_num,\n",
    "                                   ordinal_ohe=X_cat[config_data[nominal]],\n",
    "                                   ordinal_le=X_cat[config_data[ordinal]])   \n",
    "        \n",
    "        X_clean, scaler = self._standardize_Data(data=X_)\n",
    "        \n",
    "        return X_clean, self.y\n",
    "                      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "    We are generated dataset with filter method of feature selection, <br>\n",
    "    not the other two (Lasso or Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_ = _PreprocessingData()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_rf, y_train = preprocessor_._handling_data(data=train_set, \n",
    "                                             encoding='label_encoder')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "le_encoder = util.pickle_load(config_data['le_encoder_path'])\n",
    "scaler = util.pickle_load(config_data['scaler'])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_rf, y_valid = preprocessor_._handling_data(data=valid_set,\n",
    "                                                    encoding='label_encoder',\n",
    "                                                    label_encod=le_encoder,\n",
    "                                                    standardscaler=scaler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_rf, y_test = preprocessor_._handling_data(data=test_set,\n",
    "                                                encoding='label_encoder',\n",
    "                                                label_encod=le_encoder,\n",
    "                                                standardscaler=scaler)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping and Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.pickle_dump(X_train_rf, config_data[\"train_set_clean\"][0])\n",
    "util.pickle_dump(y_train, config_data[\"train_set_clean\"][1])\n",
    "\n",
    "util.pickle_dump(X_valid_rf, config_data[\"valid_set_clean\"][0])\n",
    "util.pickle_dump(y_valid, config_data[\"valid_set_clean\"][1])\n",
    "\n",
    "util.pickle_dump(X_test_rf, config_data[\"test_set_clean\"][0])\n",
    "util.pickle_dump(y_test, config_data[\"test_set_clean\"][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

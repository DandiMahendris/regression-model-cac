{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import src.util as util\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder, StandardScaler\n",
    "from src import _PreprocessingData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "config_data = util.load_config()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collect Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(config_data: dict, config: str) -> pd.DataFrame:\n",
    "    # Load set of data\n",
    "    X_train = util.pickle_load(config_data[\"train_set_eda\"][0])\n",
    "    y_train = util.pickle_load(config_data[\"train_set_eda\"][1])\n",
    "    \n",
    "    X_valid = util.pickle_load(config_data[\"valid_set_eda\"][0])\n",
    "    y_valid = util.pickle_load(config_data[\"valid_set_eda\"][1])\n",
    "    \n",
    "    X_test = util.pickle_load(config_data[\"test_set_eda\"][0])\n",
    "    y_test = util.pickle_load(config_data[\"test_set_eda\"][1])\n",
    "    \n",
    "    dataset_train = pd.concat([X_train[config], y_train[config]], axis=1)\n",
    "    dataset_valid = pd.concat([X_valid[config], y_valid[config]], axis=1)\n",
    "    dataset_test = pd.concat([X_test[config], y_test[config]], axis=1)\n",
    "    \n",
    "    \n",
    "    return dataset_train, dataset_valid, dataset_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set_rf, valid_set_rf, test_set_rf = load_dataset(config_data, config='rf')\n",
    "train_set_filter, valid_set_filter, test_set_filter = load_dataset(config_data, config='filter')\n",
    "train_set_lasso, valid_set_lasso, test_set_lasso = load_dataset(config_data, config='lasso')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Check Data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Function Preprocessing data\n",
    "\n",
    "- Split dataset into numerical and categorical data, <br>\n",
    "- Performed **SimpleImputer** in order of missing data <br>\n",
    "- Performed **Encoding** for categorical data such as **LabelEncoder** for ordinal data and **OHE** for non-ordinal data <br>\n",
    "- Suggested to use **OrdinalEncoder** and defined the ranked value of each features <br>\n",
    "- Last, performed **Standardization** using StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class _handling_data:\n",
    "    def __init__(self):\n",
    "        pass\n",
    "        \n",
    "    def _split_xy(self, data:dict) -> pd.DataFrame:\n",
    "        X_data = data.drop(columns = config_data['label'], axis=1)\n",
    "        y_data = data[config_data['label']]\n",
    "        \n",
    "        self.X = X_data\n",
    "        self.y = y_data\n",
    "        \n",
    "        numerical_col = X_data.select_dtypes('float64').columns.to_list()\n",
    "        categorical_col = X_data.select_dtypes('object').columns.to_list()\n",
    "\n",
    "        X_num = X_data[numerical_col]\n",
    "        X_cat = X_data[categorical_col]\n",
    "\n",
    "        return  X_num, X_cat\n",
    "    \n",
    "    # Perform sanity check\n",
    "    def _imputer_Num(self, data, imputer=None):\n",
    "        if imputer == None:\n",
    "            imputer = SimpleImputer(missing_values=np.nan,\n",
    "                                    strategy='median')\n",
    "            imputer.fit(data)\n",
    "\n",
    "        data_imputed_num = pd.DataFrame(imputer.transform(data),\n",
    "                                    index = data.index,\n",
    "                                    columns = data.columns)\n",
    "        \n",
    "        data_imputed_num = data_imputed_num.astype('int64')\n",
    "        \n",
    "        self.data_imputed_num = data_imputed_num\n",
    "        \n",
    "        return data_imputed_num\n",
    "\n",
    "    def _imputer_Cat(self, data, imputer = None) -> pd.DataFrame:\n",
    "        if imputer == None:\n",
    "            imputer = SimpleImputer(missing_values=np.nan,\n",
    "                                    strategy='most_frequent')\n",
    "            imputer.fit(data)\n",
    "\n",
    "        data_imputed_cat = pd.DataFrame(imputer.transform(data),\n",
    "                                    index=data.index,\n",
    "                                    columns=data.columns\n",
    "                                    )\n",
    "        \n",
    "        self.data_imputed_cat = data_imputed_cat\n",
    "        \n",
    "        return data_imputed_cat\n",
    "    \n",
    "    def _OHE_Cat(self, data, encoder_col = None, encoder = None) -> pd.DataFrame:\n",
    "\n",
    "        if encoder == None:\n",
    "            encoder = OneHotEncoder(handle_unknown= 'ignore',\n",
    "                                    drop = 'if_binary')\n",
    "            encoder.fit(data)\n",
    "            encoder_col = encoder.get_feature_names_out(data.columns)\n",
    "\n",
    "        data_encoded = encoder.transform(data).toarray()\n",
    "        data_encoded = pd.DataFrame(data_encoded,\n",
    "                                    index=data.index,\n",
    "                                    columns=encoder_col)\n",
    "        \n",
    "        # util.pickle_dump(encoder, config_data[\"ohe_path\"])\n",
    "        \n",
    "        return data_encoded, encoder_col, encoder\n",
    "    \n",
    "    def _LE_cat(self, data, encoder = None) -> pd.DataFrame:\n",
    "        \n",
    "        if encoder == None:\n",
    "            le_encoder = LabelEncoder()\n",
    "            for col in data.columns.to_list():\n",
    "                data[col] = le_encoder.fit_transform(data[col])\n",
    "        \n",
    "        return data, le_encoder\n",
    "    \n",
    "    def _concat_data(self, nominal, ordinal_ohe=None, ordinal_le=None):\n",
    "        \n",
    "        if ordinal_ohe is not None and not ordinal_ohe.empty:\n",
    "            X_train_ohe, encoder_ohe_col, encoder_ohe = self._OHE_Cat(ordinal_ohe)\n",
    "            X_train_ = pd.concat([nominal, X_train_ohe], axis=1)\n",
    "            \n",
    "        if ordinal_le is not None and not ordinal_le.empty:\n",
    "            X_train_le, encoder_le = self._LE_cat(data=ordinal_le)\n",
    "            X_train_ = pd.concat([nominal, X_train_le], axis=1)\n",
    "        \n",
    "        if ordinal_ohe is not None and not ordinal_ohe.empty and ordinal_le is not None and not ordinal_le.empty:\n",
    "            X_train_ohe, encoder_ohe_col, encoder_ohe = self._OHE_Cat(ordinal_ohe)\n",
    "            X_train_le, encoder_le = self._LE_cat(data=ordinal_le)\n",
    "            \n",
    "            X_train_cat_concat = pd.concat([X_train_ohe, X_train_le], axis=1)\n",
    "            X_train_ = pd.concat([nominal, X_train_cat_concat], axis=1)\n",
    "            \n",
    "        return X_train_\n",
    "    \n",
    "    def _standardize_Data(self, data, scaler=None) -> pd.DataFrame:\n",
    "        if scaler == None:\n",
    "            scaler = StandardScaler()\n",
    "            scaler.fit(data)\n",
    "\n",
    "        data_scaled = pd.DataFrame(scaler.transform(data),\n",
    "                                index=data.index,\n",
    "                                columns=data.columns)\n",
    "        \n",
    "        return data_scaled, scaler\n",
    "    \n",
    "    def _handling_data(self, data, encoding='le'):\n",
    "        \n",
    "        X_num, X_cat = self._split_xy(data)\n",
    "        \n",
    "        X_num = self._imputer_Num(data=X_num)\n",
    "        X_cat = self._imputer_Cat(data=X_cat)\n",
    "        \n",
    "        if encoding == 'le':\n",
    "            X_ = self._concat_data(nominal=X_num,\n",
    "                                   ordinal_le=X_cat)\n",
    "            \n",
    "        elif encoding == 'ohe':\n",
    "            X_ = self._concat_data(nominal=X_num,\n",
    "                                   ordinal_ohe=X_cat)\n",
    "        \n",
    "        else:\n",
    "            X_ = self._concat_data(nominal=X_num,\n",
    "                                   ordinal_ohe=X_cat[config_data[nominal]],\n",
    "                                   ordinal_le=X_cat[config_data[ordinal]])   \n",
    "        \n",
    "        X_clean, scaler = self._standardize_Data(data=X_)\n",
    "        \n",
    "        return X_clean, self.y\n",
    "                      "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data\n",
    "\n",
    "    We are generated dataset with filter method of feature selection, <br>\n",
    "    not the other two (Lasso or Random Forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor_ = _PreprocessingData()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 15:55:59.132839 Split numeric and categoric data\n",
      "2023-06-15 15:55:59.156806 Perform imputer.\n",
      "2023-06-15 15:55:59.304382 Perform label encoding.\n",
      "2023-06-15 15:55:59.397132 Perform Standardizing data.\n",
      "2023-06-15 15:55:59.460962 Split numeric and categoric data\n",
      "2023-06-15 15:55:59.480910 Perform imputer.\n",
      "2023-06-15 15:55:59.596676 Perform label encoding.\n",
      "2023-06-15 15:55:59.692421 Perform Standardizing data.\n",
      "2023-06-15 15:55:59.732314 Split numeric and categoric data\n",
      "2023-06-15 15:55:59.746276 Perform imputer.\n",
      "2023-06-15 15:55:59.848004 Perform label encoding.\n",
      "2023-06-15 15:55:59.927795 Perform Standardizing data.\n"
     ]
    }
   ],
   "source": [
    "X_train_rf, y_train_rf = preprocessor_._handling_data(data=train_set_rf, \n",
    "                                             encoding='label_encoder',\n",
    "                                             method='random_forest')\n",
    "\n",
    "X_train_lasso, y_train_lasso = preprocessor_._handling_data(data=train_set_lasso,\n",
    "                                                            encoding='label_encoder',\n",
    "                                                            method='lasso')\n",
    "\n",
    "X_train_filter, y_train_filter = preprocessor_._handling_data(data=train_set_filter,\n",
    "                                                              encoding='label_encoder',\n",
    "                                                              method='filter')\n",
    "                                                            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = {\n",
    "    \"filter\" : X_train_filter,\n",
    "    \"lasso\" : X_train_lasso,\n",
    "    \"rf\" : X_train_rf\n",
    "    }\n",
    "\n",
    "y_train = {\n",
    "    \"filter\" : y_train_filter,\n",
    "    \"lasso\" : y_train_lasso,\n",
    "    \"rf\" : y_train_rf\n",
    "    }"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Create Function to handle valid and test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _Concat_Preprocessing(data_filter, data_lasso, data_rf):\n",
    "\n",
    "    le_encoder_rf = util.pickle_load(config_data['le_encoder_path_rf'])\n",
    "    scaler_rf = util.pickle_load(config_data['scaler_rf'])\n",
    "\n",
    "    le_encoder_lasso = util.pickle_load(config_data['le_encoder_path_lasso'])\n",
    "    scaler_lasso = util.pickle_load(config_data['scaler_lasso'])\n",
    "\n",
    "    le_encoder_filter = util.pickle_load(config_data['le_encoder_path_filter'])\n",
    "    scaler_filter = util.pickle_load(config_data['scaler_filter'])\n",
    "    \n",
    "    X_rf, y_rf = preprocessor_._handling_data(data=data_rf, \n",
    "                                             encoding='label_encoder',\n",
    "                                             label_encod=util.pickle_load(config_data['le_encoder_path_rf']),\n",
    "                                             standard_scaler=util.pickle_load(config_data['scaler_rf'])\n",
    "                                             )\n",
    "\n",
    "    X_lasso, y_lasso = preprocessor_._handling_data(data=data_lasso,\n",
    "                                                    encoding='label_encoder',\n",
    "                                                    label_encod=util.pickle_load(config_data['le_encoder_path_lasso']),\n",
    "                                                    standard_scaler=util.pickle_load(config_data['scaler_lasso'])\n",
    "                                                    )\n",
    "\n",
    "    X_filter, y_filter = preprocessor_._handling_data(data=data_filter,\n",
    "                                                        encoding='label_encoder',\n",
    "                                                        label_encod=util.pickle_load(config_data['le_encoder_path_filter']),\n",
    "                                                        standard_scaler=util.pickle_load(config_data['scaler_filter'])\n",
    "                                                        )\n",
    "    \n",
    "    X = {\n",
    "        \"filter\" : X_filter,\n",
    "        \"lasso\" : X_lasso,\n",
    "        \"rf\" : X_rf\n",
    "    }\n",
    "    \n",
    "    y = {\n",
    "        \"filter\" : y_filter,\n",
    "        \"lasso\" : y_lasso,\n",
    "        \"rf\" : y_rf\n",
    "    }\n",
    "    \n",
    "    return X, y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 15:56:29.411381 Split numeric and categoric data\n",
      "2023-06-15 15:56:29.420360 Perform imputer.\n",
      "2023-06-15 15:56:29.460252 Perform label encoding.\n",
      "2023-06-15 15:56:29.504134 Perform Standardizing data.\n",
      "2023-06-15 15:56:29.519094 Split numeric and categoric data\n",
      "2023-06-15 15:56:29.525079 Perform imputer.\n",
      "2023-06-15 15:56:29.553999 Perform label encoding.\n",
      "2023-06-15 15:56:29.575942 Perform Standardizing data.\n",
      "2023-06-15 15:56:29.584918 Split numeric and categoric data\n",
      "2023-06-15 15:56:29.588908 Perform imputer.\n",
      "2023-06-15 15:56:29.609849 Perform label encoding.\n",
      "2023-06-15 15:56:29.630794 Perform Standardizing data.\n"
     ]
    }
   ],
   "source": [
    "X_valid, y_valid = _Concat_Preprocessing(data_filter=valid_set_filter,\n",
    "                                         data_lasso=valid_set_lasso,\n",
    "                                         data_rf=valid_set_rf\n",
    "                                         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    Test Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-15 15:56:30.701568 Split numeric and categoric data\n",
      "2023-06-15 15:56:30.707553 Perform imputer.\n",
      "2023-06-15 15:56:30.747447 Perform label encoding.\n",
      "2023-06-15 15:56:30.789335 Perform Standardizing data.\n",
      "2023-06-15 15:56:30.833217 Split numeric and categoric data\n",
      "2023-06-15 15:56:30.839202 Perform imputer.\n",
      "2023-06-15 15:56:30.875106 Perform label encoding.\n",
      "2023-06-15 15:56:30.900038 Perform Standardizing data.\n",
      "2023-06-15 15:56:30.909016 Split numeric and categoric data\n",
      "2023-06-15 15:56:30.914001 Perform imputer.\n",
      "2023-06-15 15:56:30.940933 Perform label encoding.\n",
      "2023-06-15 15:56:30.961874 Perform Standardizing data.\n"
     ]
    }
   ],
   "source": [
    "X_test, y_test = _Concat_Preprocessing(data_filter = test_set_filter,\n",
    "                                         data_lasso = test_set_lasso,\n",
    "                                         data_rf = test_set_rf\n",
    "                                         )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dumping and Save Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "util.pickle_dump(X_train, config_data[\"train_set_clean\"][0])\n",
    "util.pickle_dump(y_train, config_data[\"train_set_clean\"][1])\n",
    "\n",
    "util.pickle_dump(X_valid, config_data[\"valid_set_clean\"][0])\n",
    "util.pickle_dump(y_valid, config_data[\"valid_set_clean\"][1])\n",
    "\n",
    "util.pickle_dump(X_test, config_data[\"test_set_clean\"][0])\n",
    "util.pickle_dump(y_test, config_data[\"test_set_clean\"][1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
